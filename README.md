# LangChain Chatbot with Retrieval‑Augmented Generation (RAG)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/langchain-0.2%2B-green.svg)](https://github.com/langchain-ai/langchain)

## 📖 Overview

`langchain-chatbot` is a **reference implementation** of a conversational AI assistant built on top of **LangChain** and powered by **Retrieval‑Augmented Generation (RAG)**. The bot can:
- Answer questions using a private knowledge base (PDFs, markdown, CSV, …).
- Keep context across turns using LangChain's `ConversationChain`.
- Switch seamlessly between **retrieval** (searching the vector store) and **generation** (LLM response) based on the user query.

The repository showcases best practices for:
- Prompt engineering with LangChain `ChatPromptTemplate`.
- Vector store management (FAISS, Chroma, Pinecone, …).
- Streaming responses and tool‑use (e.g., web‑search, calculator).
- Testing and CI for LLM‑centric code.

---

## ✨ Features

| Feature | Description |
|---------|-------------|
| **RAG pipeline** | Documents → embeddings → vector store → similarity search → LLM generation |
| **Multi‑modal support** | Text, PDFs, markdown, CSV – easy to plug new loaders |
| **Chat memory** | `ConversationBufferMemory` keeps dialogue context |
| **Streaming output** | Real‑time token streaming to the UI (FastAPI/WebSocket) |
| **Modular architecture** | Separate packages for `retrievers`, `llms`, `prompts`, `api` |
| **Extensible** | Swap LLMs (OpenAI, Anthropic, Ollama, HuggingFace) and vector stores with a single config file |
| **Testing utilities** | Mock LLMs and vector stores for unit tests |
| **Docker support** | Ready‑to‑run container for reproducible environments |

---

## 🚀 Quick Start

### Prerequisites

- Python **3.9+**
- An LLM API key (OpenAI, Anthropic, etc.)
- (Optional) `docker` & `docker‑compose` for containerised run

### 1. Clone the repository

```bash
git clone https://github.com/your-org/langchain-chatbot.git
cd langchain-chatbot
```

### 2. Install dependencies

```bash
# Using poetry (recommended)
poetry install
# Or with pip
pip install -r requirements.txt
```

### 3. Set environment variables

Create a `.env` file at the project root:

```dotenv
# LLM provider – e.g., openai, anthropic, ollama
LLM_PROVIDER=openai
# Corresponding API key
OPENAI_API_KEY=sk-*****
# Choose a vector store (faiss, chroma, pinecone)
VECTOR_STORE=faiss
# Path to your knowledge‑base folder (see docs/knowledge/)
DOCS_PATH=./docs/knowledge
```

### 4. Load your knowledge base

```bash
python scripts/load_documents.py --source ./docs/knowledge
```
This will:
1. Crawl the folder, extract text with LangChain loaders.
2. Compute embeddings (default `OpenAIEmbeddings`).
3. Persist the vector store under `./vector_store/`.

### 5. Run the chatbot server

```bash
uvicorn app.main:app --reload
```
Open your browser at `http://localhost:8000/docs` to interact via the autogenerated FastAPI UI, or integrate the WebSocket endpoint into your front‑end.

---

## 🛠️ Installation (Library usage)

If you only need the **Python package** (e.g., to embed the bot in another project):

```bash
pip install langchain-chatbot
```

```python
from langchain_chatbot import Chatbot, Config

cfg = Config(
    llm_provider="openai",
    api_key="sk-...",
    vector_store="faiss",
    docs_path="./my_docs",
)
bot = Chatbot(cfg)
response = bot.ask("Explain the benefits of RAG.")
print(response)
```

---

## 📂 Project Structure

```
langchain-chatbot/
├─ app/                     # FastAPI entry point & WebSocket handlers
│   ├─ main.py
│   └─ routes.py
├─ langchain_chatbot/       # Core library
│   ├─ __init__.py
│   ├─ bot.py               # High‑level Chatbot class
│   ├─ config.py            # Pydantic settings
│   ├─ retrievers/          # Vector store wrappers
│   ├─ llms/                # LLM adapters (OpenAI, Anthropic, …)
│   └─ prompts/             # Prompt templates & utilities
├─ scripts/                 # Helper scripts (document loading, indexing)
│   └─ load_documents.py
├─ tests/                   # Unit & integration tests
│   └─ test_bot.py
├─ docs/                    # Example knowledge‑base files
├─ .env.example
├─ pyproject.toml / requirements.txt
└─ README.md                # ← you are here
```

---

## 🧩 How RAG Works in This Repo

1. **Document ingestion** – LangChain loaders (`UnstructuredFileLoader`, `PDFMinerLoader`, …) convert raw files to plain text.
2. **Chunking** – Text is split with `RecursiveCharacterTextSplitter` (default chunk size 1000, overlap 200).
3. **Embedding** – Each chunk is turned into a dense vector using the configured embedding model.
4. **Vector store** – Vectors are stored in a chosen backend (FAISS by default) and persisted on disk.
5. **Retrieval** – At query time, the user prompt is embedded and the *k* most similar chunks are fetched.
6. **Prompt composition** – Retrieved passages are injected into a `ChatPromptTemplate` along with the conversation history.
7. **Generation** – The LLM produces a response that is both **grounded** in the retrieved context and **aware** of the dialogue state.

The separation of concerns makes it trivial to swap any stage (e.g., replace FAISS with Pinecone, or use `SentenceTransformers` embeddings).

---

## 🤝 Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository and create a feature branch.
2. Ensure code style with `ruff` and type‑check with `mypy`.
3. Add or update tests in the `tests/` folder. Run the full suite with:
   ```bash
   pytest -q
   ```
4. Update the documentation (`README.md` or docstrings) as needed.
5. Submit a Pull Request with a clear description of the change.

### Development workflow

```bash
# Install dev dependencies
poetry install --with dev
# Run the linter & type checker
ruff check .
mypy langchain_chatbot
# Run tests
pytest
```

### Code of Conduct

Please adhere to the [Contributor Covenant Code of Conduct](CODE_OF_CONDUCT.md).

---

## 📦 Release & Versioning

We use **Semantic Versioning** (`MAJOR.MINOR.PATCH`). To publish a new version:

```bash
poetry version minor   # or patch / major
git tag -a vX.Y.Z -m "Release vX.Y.Z"
git push --tags
poetry build && poetry publish
```

---

## 📄 License

This project is licensed under the **MIT License** – see the [LICENSE](LICENSE) file for details.

---

## 🙏 Acknowledgements

- The **LangChain** community for the powerful abstractions.
- **OpenAI**, **Anthropic**, and other LLM providers for their APIs.
- Contributors of the underlying vector‑store libraries (FAISS, Chroma, Pinecone).

---

*Happy building with LangChain!*