# LangChain Chatbot with Retrievalâ€‘Augmented Generation (RAG)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/langchain-0.2%2B-green.svg)](https://github.com/langchain-ai/langchain)

## ğŸ“– Overview

`langchain-chatbot` is a **reference implementation** of a conversational AI assistant built on top of **LangChain** and powered by **Retrievalâ€‘Augmented Generation (RAG)**. The bot can:
- Answer questions using a private knowledge base (PDFs, markdown, CSV, â€¦).
- Keep context across turns using LangChain's `ConversationChain`.
- Switch seamlessly between **retrieval** (searching the vector store) and **generation** (LLM response) based on the user query.

The repository showcases best practices for:
- Prompt engineering with LangChain `ChatPromptTemplate`.
- Vector store management (FAISS, Chroma, Pinecone, â€¦).
- Streaming responses and toolâ€‘use (e.g., webâ€‘search, calculator).
- Testing and CI for LLMâ€‘centric code.

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **RAG pipeline** | Documents â†’ embeddings â†’ vector store â†’ similarity search â†’ LLM generation |
| **Multiâ€‘modal support** | Text, PDFs, markdown, CSV â€“ easy to plug new loaders |
| **Chat memory** | `ConversationBufferMemory` keeps dialogue context |
| **Streaming output** | Realâ€‘time token streaming to the UI (FastAPI/WebSocket) |
| **Modular architecture** | Separate packages for `retrievers`, `llms`, `prompts`, `api` |
| **Extensible** | Swap LLMs (OpenAI, Anthropic, Ollama, HuggingFace) and vector stores with a single config file |
| **Testing utilities** | Mock LLMs and vector stores for unit tests |
| **Docker support** | Readyâ€‘toâ€‘run container for reproducible environments |

---

## ğŸš€ Quick Start

### Prerequisites

- Python **3.9+**
- An LLM API key (OpenAI, Anthropic, etc.)
- (Optional) `docker` & `dockerâ€‘compose` for containerised run

### 1. Clone the repository

```bash
git clone https://github.com/your-org/langchain-chatbot.git
cd langchain-chatbot
```

### 2. Install dependencies

```bash
# Using poetry (recommended)
poetry install
# Or with pip
pip install -r requirements.txt
```

### 3. Set environment variables

Create a `.env` file at the project root:

```dotenv
# LLM provider â€“ e.g., openai, anthropic, ollama
LLM_PROVIDER=openai
# Corresponding API key
OPENAI_API_KEY=sk-*****
# Choose a vector store (faiss, chroma, pinecone)
VECTOR_STORE=faiss
# Path to your knowledgeâ€‘base folder (see docs/knowledge/)
DOCS_PATH=./docs/knowledge
```

### 4. Load your knowledge base

```bash
python scripts/load_documents.py --source ./docs/knowledge
```
This will:
1. Crawl the folder, extract text with LangChain loaders.
2. Compute embeddings (default `OpenAIEmbeddings`).
3. Persist the vector store under `./vector_store/`.

### 5. Run the chatbot server

```bash
uvicorn app.main:app --reload
```
Open your browser at `http://localhost:8000/docs` to interact via the autogenerated FastAPI UI, or integrate the WebSocket endpoint into your frontâ€‘end.

---

## ğŸ› ï¸ Installation (Library usage)

If you only need the **Python package** (e.g., to embed the bot in another project):

```bash
pip install langchain-chatbot
```

```python
from langchain_chatbot import Chatbot, Config

cfg = Config(
    llm_provider="openai",
    api_key="sk-...",
    vector_store="faiss",
    docs_path="./my_docs",
)
bot = Chatbot(cfg)
response = bot.ask("Explain the benefits of RAG.")
print(response)
```

---

## ğŸ“‚ Project Structure

```
langchain-chatbot/
â”œâ”€ app/                     # FastAPI entry point & WebSocket handlers
â”‚   â”œâ”€ main.py
â”‚   â””â”€ routes.py
â”œâ”€ langchain_chatbot/       # Core library
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ bot.py               # Highâ€‘level Chatbot class
â”‚   â”œâ”€ config.py            # Pydantic settings
â”‚   â”œâ”€ retrievers/          # Vector store wrappers
â”‚   â”œâ”€ llms/                # LLM adapters (OpenAI, Anthropic, â€¦)
â”‚   â””â”€ prompts/             # Prompt templates & utilities
â”œâ”€ scripts/                 # Helper scripts (document loading, indexing)
â”‚   â””â”€ load_documents.py
â”œâ”€ tests/                   # Unit & integration tests
â”‚   â””â”€ test_bot.py
â”œâ”€ docs/                    # Example knowledgeâ€‘base files
â”œâ”€ .env.example
â”œâ”€ pyproject.toml / requirements.txt
â””â”€ README.md                # â† you are here
```

---

## ğŸ§© How RAG Works in This Repo

1. **Document ingestion** â€“ LangChain loaders (`UnstructuredFileLoader`, `PDFMinerLoader`, â€¦) convert raw files to plain text.
2. **Chunking** â€“ Text is split with `RecursiveCharacterTextSplitter` (default chunk size 1000, overlap 200).
3. **Embedding** â€“ Each chunk is turned into a dense vector using the configured embedding model.
4. **Vector store** â€“ Vectors are stored in a chosen backend (FAISS by default) and persisted on disk.
5. **Retrieval** â€“ At query time, the user prompt is embedded and the *k* most similar chunks are fetched.
6. **Prompt composition** â€“ Retrieved passages are injected into a `ChatPromptTemplate` along with the conversation history.
7. **Generation** â€“ The LLM produces a response that is both **grounded** in the retrieved context and **aware** of the dialogue state.

The separation of concerns makes it trivial to swap any stage (e.g., replace FAISS with Pinecone, or use `SentenceTransformers` embeddings).

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository and create a feature branch.
2. Ensure code style with `ruff` and typeâ€‘check with `mypy`.
3. Add or update tests in the `tests/` folder. Run the full suite with:
   ```bash
   pytest -q
   ```
4. Update the documentation (`README.md` or docstrings) as needed.
5. Submit a Pull Request with a clear description of the change.

### Development workflow

```bash
# Install dev dependencies
poetry install --with dev
# Run the linter & type checker
ruff check .
mypy langchain_chatbot
# Run tests
pytest
```

### Code of Conduct

Please adhere to the [Contributor Covenant Code of Conduct](CODE_OF_CONDUCT.md).

---

## ğŸ“¦ Release & Versioning

We use **Semantic Versioning** (`MAJOR.MINOR.PATCH`). To publish a new version:

```bash
poetry version minor   # or patch / major
git tag -a vX.Y.Z -m "Release vX.Y.Z"
git push --tags
poetry build && poetry publish
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgements

- The **LangChain** community for the powerful abstractions.
- **OpenAI**, **Anthropic**, and other LLM providers for their APIs.
- Contributors of the underlying vectorâ€‘store libraries (FAISS, Chroma, Pinecone).

---

*Happy building with LangChain!*