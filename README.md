# LangChain Chatbot with Retrieval‑Augmented Generation (RAG)

## 📖 Overview

This repository provides a **minimal yet production‑ready** example of a chatbot built on top of **[LangChain](https://github.com/langchain-ai/langchain)**.  The bot demonstrates how to combine:

- **LLM inference** (OpenAI, Anthropic, Llama 2, etc.)
- **Document retrieval** using vector stores (FAISS, Chroma, Pinecone…)
- **RAG (Retrieval‑Augmented Generation)** to ground responses in external knowledge

The goal is to give developers a clear, extensible starting point for building conversational agents that can answer questions with up‑to‑date, source‑referenced information.

---

## 🛠️ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/your‑org/langchain-chatbot.git
cd langchain-chatbot
```

### 2. Install dependencies

We use **Poetry** for deterministic dependency management, but a `requirements.txt` is also provided for pip users.

#### Using Poetry (recommended)

```bash
poetry install
poetry shell
```

#### Using pip

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file at the project root (or export variables in your shell). The minimal set is:

```dotenv
# LLM provider – e.g. openai, anthropic, huggingface
LLM_PROVIDER=openai
# API key for the chosen provider
OPENAI_API_KEY=sk-...
# Optional: model name (default depends on provider)
OPENAI_MODEL=gpt-4o-mini
# Vector store – faiss, chroma, pinecone, weaviate, etc.
VECTOR_STORE=faiss
# Path where the FAISS index will be persisted (if using FAISS)
FAISS_INDEX_PATH=./data/faiss_index
```

> **Tip:** For local LLMs you can set `LLM_PROVIDER=local` and point `LOCAL_MODEL_PATH` to a HuggingFace model directory.

### 4. Populate the knowledge base

Place the documents you want the bot to reference in the `data/documents/` folder. Supported formats include:

- `.txt`
- `.md`
- `.pdf`
- `.docx`

Then run the ingestion script to build the vector store:

```bash
python scripts/ingest.py
```

The script will:
1. Load files from `data/documents/`
2. Split them into chunks (default: 1000 tokens, 200‑token overlap)
3. Embed each chunk using the selected embedding model (OpenAI `text-embedding-ada-002` by default)
4. Persist the index to the location configured in `.env`

### 5. Launch the chatbot

```bash
python app.py
```

Open `http://127.0.0.1:8000` in your browser. You will see a simple UI built with **Gradio** (or FastAPI + React if you enable the optional front‑end).

---

## 📂 Project Structure

```
langchain-chatbot/
│
├─ app.py                 # Entry point – FastAPI/Gradio server
├─ chain.py               # LangChain RAG pipeline definition
├─ prompts/               # Prompt templates (system, user, etc.)
│   └─ rag_prompt.txt
├─ scripts/               # Utility scripts (ingest, eval, etc.)
│   └─ ingest.py
├─ data/
│   ├─ documents/         # Raw knowledge‑base files (add yours here)
│   └─ faiss_index/       # Persisted vector store (generated by ingest)
├─ tests/                 # Unit / integration tests
│   └─ test_chain.py
├─ .env.example           # Example environment file
├─ requirements.txt       # pip‑compatible dependencies
├─ pyproject.toml         # Poetry configuration
└─ README.md              # <-- You are reading it!
```

---

## 🧩 Core Components

### `chain.py`
Defines the **RAG chain** using LangChain primitives:
```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# 1️⃣ Load vector store
vectorstore = FAISS.load_local(FAISS_INDEX_PATH, OpenAIEmbeddings())

# 2️⃣ Create retriever (top‑k docs)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 3️⃣ LLM – configurable via env
llm = OpenAI(model_name=os.getenv("OPENAI_MODEL"))

# 4️⃣ RetrievalQA chain with a custom prompt
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": rag_prompt},
)
```
The chain returns both the generated answer **and** the source documents, enabling transparent citations.

### Prompt Engineering
A well‑crafted system prompt improves factuality. The default `prompts/rag_prompt.txt` contains:
```
You are a helpful assistant that answers questions using ONLY the provided context.
If the answer cannot be derived from the context, respond with "I don't know".
Cite the source document IDs after each statement.
```
Feel free to adapt it to your domain.

---

## 🤝 Contributing

Contributions are welcome! Follow these steps:

1. **Fork** the repository.
2. Create a feature branch:
   ```bash
   git checkout -b feat/your-feature
   ```
3. Write tests for new functionality (see `tests/`).
4. Ensure the test suite passes:
   ```bash
   pytest -q
   ```
5. Open a Pull Request with a clear description of the change.

### Development Guidelines
- Keep the public API of `chain.py` stable.
- Use type hints throughout the codebase.
- Follow **PEP 8** and **Black** formatting (`black .`).
- Update the documentation (README, docstrings) when adding features.

---

## 📚 Further Reading & Resources

- **LangChain Documentation** – https://python.langchain.com/
- **RAG Primer** – https://arxiv.org/abs/2005.11401
- **Vector Store Comparison** – https://github.com/langchain-ai/langchain/blob/master/docs/modules/data_connection/vectorstores.md
- **Prompt Design** – https://github.com/prompt-engineering-guidelines

---

## 📄 License

This project is licensed under the **MIT License**. See the `LICENSE` file for details.

---

*Happy building!*