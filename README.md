# LangChain Chatbot with Retrievalâ€‘Augmented Generation (RAG)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9%2B-brightgreen.svg)](https://www.python.org/)

A lightweight, extensible chatbot built on **[LangChain](https://github.com/hwchase17/langchain)** that demonstrates **Retrievalâ€‘Augmented Generation (RAG)**.  The bot can answer questions over a custom knowledge base, combine LLM reasoning with vectorâ€‘store retrieval, and be easily extended for new data sources or LLM providers.

---

## ğŸ“– Table of Contents

- [Features](#-features)
- [Demo](#-demo)
- [Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Configuration](#configuration)
- [Running the Bot](#-running-the-bot)
- [Project Structure](#-project-structure)
- [Extending & Contributing](#-extending--contributing)
- [Testing](#-testing)
- [License](#-license)
- [Acknowledgements](#-acknowledgements)

---

## âœ¨ Features

- **RAG pipeline** â€“ combines a **vector store** (FAISS, Chroma, Pineconeâ€¦) with a **LLM** (OpenAI, Anthropic, Llamaâ€‘2â€¦) to answer queries grounded in your own documents.
- **Modular design** â€“ interchangeable components for document loaders, embeddings, vector stores, and LLMs.
- **Prompt engineering** â€“ builtâ€‘in system prompts and chat history handling.
- **CLI & API** â€“ run locally via a simple CLI or expose a FastAPI endpoint.
- **Docker support** â€“ optional `Dockerfile` for reproducible environments.
- **Extensive tests** â€“ unit and integration tests using `pytest` and `pytestâ€‘asyncio`.

---

## ğŸ¥ Demo

```bash
# After installation (see below)
python -m chatbot.cli
```

You will be prompted for a question. The bot will retrieve the most relevant chunks from the knowledge base and generate a response using the selected LLM.

---

## ğŸš€ Getting Started

### Prerequisites

| Tool | Version |
|------|---------|
| Python | >=3.9 |
| pip | >=21.0 |
| Git | any |
| (Optional) Docker | >=20.10 |

You also need an API key for the LLM you intend to use (e.g., `OPENAI_API_KEY`).

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/langchain-chatbot.git
cd langchain-chatbot

# Create a virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install the package in editable mode
pip install -e .[dev]
```

The optional `dev` extras install testing and linting tools.

### Configuration

Create a `.env` file at the project root (or export environment variables directly):

```dotenv
# LLM provider â€“ currently supported: openai, anthropic, huggingface
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
# Embedding model â€“ e.g., openaiâ€‘textâ€‘embeddingâ€‘adaâ€‘002
EMBEDDING_MODEL=text-embedding-ada-002
# Vector store â€“ faiss, chroma, pinecone, etc.
VECTOR_STORE=faiss
# Path to the directory containing source documents (pdf, txt, md, etc.)
DOCS_PATH=./data
```

You can also override defaults via commandâ€‘line arguments (see the CLI help).

---

## ğŸƒ Running the Bot

### CLI

```bash
python -m chatbot.cli --docs ./data --store faiss
```

The CLI will:
1. Load documents from `DOCS_PATH` (or the path you provide).
2. Split them into chunks using LangChain's `RecursiveCharacterTextSplitter`.
3. Embed the chunks and store them in the selected vector store.
4. Start an interactive prompt where you can type questions.

### FastAPI server (optional)

```bash
uvicorn chatbot.api:app --reload
```

The API exposes a single endpoint:
- `POST /chat` â€“ body `{ "question": "..." }` â€“ returns `{ "answer": "...", "sources": [...] }`.

---

## ğŸ“ Project Structure

```
langchain-chatbot/
â”œâ”€ chatbot/                     # Core package
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ config.py                # Pydantic settings (loads .env)
â”‚   â”œâ”€ loaders/                 # Document loaders (pdf, txt, markdownâ€¦)
â”‚   â”œâ”€ pipelines/               # RAG pipeline implementation
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ rag.py                # Highâ€‘level RAG class
â”‚   â”œâ”€ llms/                    # Wrapper classes for OpenAI, Anthropic, etc.
â”‚   â”œâ”€ stores/                  # Vectorâ€‘store adapters (FAISS, Chromaâ€¦)
â”‚   â”œâ”€ cli.py                   # Clickâ€‘based command line interface
â”‚   â””â”€ api.py                   # FastAPI entry point
â”œâ”€ tests/                       # Unit & integration tests
â”‚   â”œâ”€ test_rag.py
â”‚   â””â”€ ...
â”œâ”€ data/                        # Example documents (gitâ€‘ignored in real project)
â”œâ”€ .env.example                 # Template for environment variables
â”œâ”€ pyproject.toml               # Poetry/PEPâ€‘517 build config
â”œâ”€ requirements.txt             # Pinâ€‘down runtime deps (generated)
â””â”€ README.md                    # â† this file
```

---

## ğŸ› ï¸ Extending & Contributing

### Adding a new document loader
1. Create a class in `chatbot/loaders/` that inherits from `BaseLoader`.
2. Implement a `load()` method returning a list of `Document` objects.
3. Register the loader in `chatbot/pipelines/rag.py` (or expose via CLI flag).

### Supporting a new vector store
1. Implement a subclass of `BaseVectorStore` in `chatbot/stores/`.
2. Provide `add_texts`, `similarity_search`, and `save/load` methods.
3. Add a corresponding entry in the `VECTOR_STORE` enum in `config.py`.

### Development workflow
```bash
# Run the test suite
pytest -q
# Lint & format
ruff check .
black .
```

Pull requests should:
- Include tests for new functionality.
- Update the documentation (README or docstrings) as needed.
- Pass all CI checks.

---

## âœ… Testing

The repository ships with a comprehensive test suite using `pytest`.  To run:

```bash
pytest
```

For fast feedback during development, you can run only the unit tests:

```bash
pytest tests/unit
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgements

- **LangChain** â€“ the foundational framework that makes chaining LLMs and external tools effortless.
- **FAISS / Chroma** â€“ for efficient vector similarity search.
- **OpenAI / Anthropic** â€“ for the underlying large language models.
- Community contributors who helped refine the RAG workflow.

---

*Happy building! If you encounter issues or have ideas for improvement, feel free to open an issue or submit a pull request.*