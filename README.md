# LangChain Chatbot

## ğŸ“– Description

**LangChainâ€‘Chatbot** est un exemple de bot conversationnel basÃ© sur **LangChain** et la technique **RAG (Retrievalâ€‘Augmented Generation)**. Le projet montre comment combiner un modÃ¨le de gÃ©nÃ©ration de texte (LLM) avec un moteur de recherche de documents afin dâ€™enrichir les rÃ©ponses du bot avec du contexte provenant de sources externes (documents PDF, bases de connaissances, etc.).

---

## âœ¨ FonctionnalitÃ©s principales

- **IntÃ©gration LangChain** : utilisation des chaÃ®nes (`Chains`), des agents (`Agents`) et des `Retrievers`.
- **RAG complet** : indexation de documents, recherche sÃ©mantique et injection du texte rÃ©cupÃ©rÃ© dans le prompt LLM.
- **Support de multiples LLM** (OpenAI, Anthropic, Ollama, etc.) via lâ€™interface unifiÃ©e de LangChain.
- **Configuration via fichier YAML** â€“ facile Ã  adapter pour diffÃ©rents modÃ¨les, bases de donnÃ©es vectorielles et sources de documents.
- **Docker** : image prÃªte Ã  Ãªtre dÃ©ployÃ©e en local ou sur le cloud.
- **Tests unitaires** pour les composants critiques (indexation, rÃ©cupÃ©ration, chaÃ®ne de gÃ©nÃ©ration).

---

## ğŸ› ï¸ PrÃ©requis

| Outil | Version minimale |
|-------|------------------|
| Python | 3.9 |
| pip | 22.0 |
| Docker (optionnel) | 20.10 |
| Une clÃ© API pour le LLM choisi (ex. `OPENAI_API_KEY`) |

---

## ğŸš€ Installation

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/yourâ€‘org/langchain-chatbot.git
cd langchain-chatbot
```

### 2ï¸âƒ£ CrÃ©er un environnement virtuel
```bash
python -m venv .venv
source .venv/bin/activate   # sous Linux/macOS
# .venv\Scripts\activate   # sous Windows
```

### 3ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configurer les variables dâ€™environnement
CrÃ©ez un fichier `.env` Ã  la racine du projetâ€¯:
```dotenv
# Exemple pour OpenAI
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Si vous utilisez un autre LLM, ajoutez les variables correspondantes
# Exemple pour Cohere
COHERE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### 5ï¸âƒ£ (Optionnel) Lancer avec Docker
```bash
docker build -t langchainâ€‘chatbot .
docker run -p 8000:8000 --env-file .env langchainâ€‘chatbot
```

---

## ğŸ“š Utilisation

### Indexation de documents
Le script `scripts/index_documents.py` parcourt le rÃ©pertoire `data/` et crÃ©e un index vectoriel (FAISS par dÃ©faut).
```bash
python scripts/index_documents.py --source data/ --index_path indexes/faiss.idx
```

### DÃ©marrer le serveur de chatbot
```bash
uvicorn app.main:app --reload
```
Le bot sera disponible sur `http://127.0.0.1:8000/docs` (Swagger UI) oÃ¹ vous pourrez tester les endpoints.

### Exemple dâ€™appel via `curl`
```bash
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "Quelles sont les Ã©tapes pour crÃ©er un pipeline RAG avec LangChain ?"}'
```

### Interaction via interface web (facultatif)
Le rÃ©pertoire `frontend/` contient une petite application React qui consomme lâ€™API. AprÃ¨s avoir installÃ© les dÃ©pendances (`npm install`) et lancÃ© `npm start`, vous pouvez discuter avec le bot directement depuis votre navigateur.

---

## ğŸ—ï¸ Architecture du projet

```
langchain-chatbot/
â”œâ”€ app/                # API FastAPI
â”‚   â”œâ”€ main.py         # Point dâ€™entrÃ©e
â”‚   â””â”€ router.py       # Endpoints /chat, /health
â”œâ”€ core/               # Logique mÃ©tier LangChain
â”‚   â”œâ”€ rag_chain.py    # ChaÃ®ne RAG complÃ¨te
â”‚   â”œâ”€ retriever.py   # Wrapper autour du vecteur store
â”‚   â””â”€ llm_provider.py # SÃ©lection du LLM selon la config
â”œâ”€ data/               # Documents sources (PDF, txt, markdownâ€¦)
â”œâ”€ indexes/            # Indexes vectoriels gÃ©nÃ©rÃ©s
â”œâ”€ scripts/            # Outils CLI (indexation, nettoyage)
â”œâ”€ tests/              # Tests unitaires & dâ€™intÃ©gration
â”œâ”€ requirements.txt    # DÃ©pendances Python
â”œâ”€ Dockerfile          # Image Docker
â””â”€ README.md           # â† Vous Ãªtes ici
```

- **`core.rag_chain.RAGChain`** orchestreâ€¯: rÃ©cupÃ©ration â†’ formatage du contexte â†’ appel LLM.
- **`core.retriever.Retriever`** encapsule la logique du vecteur store (FAISS, Chroma, Pinecone, â€¦) et expose une mÃ©thode `get_relevant_documents(query, k)`.
- **`core.llm_provider.LLMProvider`** charge dynamiquement le modÃ¨le Ã  partir du fichier `config.yaml`.

---

## ğŸ¤ Contribuer

Les contributions sont les bienvenuesâ€¯! Voici comment commencerâ€¯:

1. **Fork** le dÃ©pÃ´t et crÃ©ez une branche feature (`git checkout -b feature/maâ€‘nouvelleâ€‘fonction`).
2. Installez les dÃ©pendances de dÃ©veloppementâ€¯:
   ```bash
   pip install -r dev-requirements.txt
   ```
3. Assurezâ€‘vous que les tests passentâ€¯:
   ```bash
   pytest -q
   ```
4. Ouvrez une **Pull Request** dÃ©crivant le problÃ¨me rÃ©solu ou la fonctionnalitÃ© ajoutÃ©e.
5. Respectez le style de codeâ€¯: `black`, `isort` et `flake8` sont configurÃ©s dans le repo.

### Guide de contribution rapide
- **Documentation**â€¯: mettez Ã  jour le `README.md` ou les docstrings si vous ajoutez de nouvelles fonctions.
- **Tests**â€¯: chaque nouvelle logique doit Ãªtre couverte par au moins un test unitaires.
- **CI**â€¯: le pipeline GitHub Actions exÃ©cute les tests et le linting automatiquement.

---

## ğŸ“„ Licence

Ce projet est distribuÃ© sous licence **MIT** â€“ voir le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- **LangChain** â€“ pour la bibliothÃ¨que puissante qui simplifie la crÃ©ation de pipelines LLM.
- **FAISS** â€“ pour le moteur de recherche vectorielle ultraâ€‘rapide.
- La communautÃ© openâ€‘source qui contribue aux modÃ¨les de langage et aux outils RAG.

---

*Pour toute question, nâ€™hÃ©sitez pas Ã  ouvrir une issue ou Ã  contacter les mainteneurs via le tableau des contributeurs.*
