# LangChain Chatbot

**LangChain‑Chatbot** is a lightweight reference implementation of a Retrieval‑Augmented Generation (RAG) chatbot built with **LangChain**, **OpenAI** (or any compatible LLM), and **FAISS** for vector similarity search.  The project demonstrates how to combine document ingestion, embedding, retrieval, and prompt engineering to create a conversational assistant that can answer questions using both its internal knowledge and external data sources.

---

## Table of Contents

- [Features](#features)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Getting Started](#getting-started)
- [Project Structure](#project-structure)
- [How It Works](#how-it-works)
- [Running the Bot](#running-the-bot)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

---

## Features

- **RAG pipeline**: ingest documents → create embeddings → store in a FAISS index → retrieve relevant chunks at query time.
- **Modular design**: each step (loader, splitter, embedder, retriever, chain) lives in its own module, making it easy to swap components.
- **OpenAI compatible**: works with any OpenAI‑compatible LLM (OpenAI, Azure OpenAI, Ollama, etc.).
- **Prompt templates**: customizable system and human prompts for fine‑tuned behavior.
- **CLI & Streamlit UI**: run the bot from the terminal or launch an interactive web UI.
- **Docker support**: containerised for reproducible environments.

---

## Prerequisites

| Requirement | Version |
|-------------|---------|
| Python      | >=3.9   |
| pip         | latest  |
| OpenAI API key (or compatible service) |
| Git         | any    |

> **Tip**: Use a virtual environment (`venv` or `conda`) to keep dependencies isolated.

---

## Installation

```bash
# Clone the repository
git clone https://github.com/your-org/langchain-chatbot.git
cd langchain-chatbot

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate   # on Windows use `.venv\Scripts\activate`

# Install dependencies
pip install -r requirements.txt

# Optional: install the development extras (testing, linting)
pip install -r dev-requirements.txt
```

### Environment variables
Create a ```.env``` file at the project root (or export the variables in your shell) with the following keys:

```dotenv
# OpenAI (or Azure/OpenAI compatible) credentials
OPENAI_API_KEY=sk-**************

# If you use Azure OpenAI, also set:
# AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2023-05-15
# AZURE_OPENAI_DEPLOYMENT_NAME=<deployment-name>

# Optional: change the default FAISS index location
FAISS_INDEX_PATH=./faiss_index
```

---

## Getting Started

### 1. Prepare your knowledge base
Place any text‑based documents (```.txt``` , ```.md``` , ```.pdf``` etc.) inside the ```.data``` folder. The repository ships with a small sample corpus.

```bash
mkdir -p data
# copy or download your documents into ./data
```

### 2. Build the vector store
Run the ingestion script to split the documents, compute embeddings, and store them in a FAISS index.

```bash
python scripts/ingest.py
```

The script will:
1. Load all files in ```.data``` using LangChain loaders.
2. Split them into chunks (default: 1000 characters, 200 overlap).
3. Generate embeddings via ``OpenAIEmbeddings`` (or any ``Embedding`` class you configure).
4. Persist the FAISS index to ``FAISS_INDEX_PATH``.

### 3. Launch the chatbot
You have two entry points:

#### a) Command‑line interface (CLI)
```bash
python src/main.py
```
Type your question and press **Enter**.  Type ``exit`` or ``quit`` to stop.

#### b) Streamlit web UI (recommended for quick demos)
```bash
streamlit run src/app.py
```
Open the displayed URL (usually http://localhost:8501) and start chatting.

---

## Project Structure

```
langchain-chatbot/
│
├─ data/                 # Your source documents (txt, md, pdf, …)
├─ faiss_index/          # Persisted FAISS index (generated by ingest.py)
├─ src/                  # Core package
│   ├─ __init__.py
│   ├─ loaders.py        # Document loaders (TextLoader, PDFLoader, …)
│   ├─ splitter.py       # Text splitter configuration
│   ├─ embeddings.py     # Embedding model wrapper
│   ├─ vector_store.py   # FAISS store helper
│   ├─ prompts.py        # System / human prompt templates
│   ├─ chain.py          # RetrievalQA chain construction
│   ├─ main.py           # CLI entry point
│   └─ app.py            # Streamlit UI entry point
│
├─ scripts/              # Utility scripts
│   └─ ingest.py         # Build/rebuild the vector store
│
├─ tests/                # Unit and integration tests
│   └─ test_*.py
│
├─ .env.example          # Example environment file
├─ requirements.txt
├─ dev-requirements.txt
└─ README.md             # <‑‑ you are reading it!
```

---

## How It Works (RAG Overview)

1. **Document Loading** – LangChain loaders read raw files and produce ``Document`` objects.
2. **Chunking** – ``RecursiveCharacterTextSplitter`` breaks each document into overlapping chunks suitable for embedding.
3. **Embedding** – ``OpenAIEmbeddings`` (or any ``Embeddings`` implementation) converts each chunk into a high‑dimensional vector.
4. **Vector Store** – FAISS stores the vectors and enables fast similarity search.
5. **Retriever** – ``FAISSRetriever`` fetches the *k* most relevant chunks for a user query.
6. **Prompt Construction** – Retrieved chunks are injected into a system prompt that instructs the LLM to answer using the provided context.
7. **LLM Generation** – The LLM produces a response, which is returned to the user.

The separation of concerns makes it trivial to replace any component (e.g., switch to Pinecone, use a local LLM, adjust chunk size, or customise the prompt).

---

## Running the Bot

Below are the most common commands you’ll use during development.

```bash
# Re‑ingest data after adding/removing documents
python scripts/ingest.py

# Start the CLI chatbot
python src/main.py

# Launch the Streamlit UI
streamlit run src/app.py

# Run the test suite
pytest
```

---

## Testing

The project includes a small test suite based on **pytest**.  Tests cover:
- Document loading and splitting
- Embedding generation (mocked for speed)
- Retrieval correctness
- End‑to‑end chain execution

Run the tests with:
```bash
pytest -v
```

CI pipelines can be added later (GitHub Actions template is in the `/.github/workflows` directory).

---

## Contributing

Contributions are welcome!  Follow these steps:
1. **Fork** the repository and clone your fork.
2. Create a feature branch: `git checkout -b feat/your-feature`.
3. Install the development dependencies (`dev-requirements.txt`).
4. Make your changes, ensuring that existing tests pass and adding new tests when appropriate.
5. Run `black .` and `ruff .` to keep the code style consistent.
6. Submit a Pull Request with a clear description of the change.

Please respect the **code of conduct** (see `CODE_OF_CONDUCT.md`).

---

## License

This project is licensed under the **MIT License** – see the `LICENSE` file for details.

---

## Acknowledgements

- **LangChain** – for the modular RAG framework.
- **OpenAI** – for the powerful language models.
- **FAISS** – for efficient similarity search.
- The open‑source community for inspiration and tooling.

---

*Happy building!*